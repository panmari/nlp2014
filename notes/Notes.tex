\documentclass[11pt]{article}
%Gummi|065|=)
\title{\textbf{Natural language processing}}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\author{Stefan Moser}
\date{\today}
\begin{document}

\maketitle
\section{Introduction}
Most information only present in natural language, not tables/machine readable form. 

\section{XML}
Instead of only text, want to share more information (structure!). 
XML = \emph{eXtensible Markup Language}, similar
to HTML, but tags are not predefined. HTML is used to display information, XML is about describing
information. First line is XML declaration (encoding, version). Node may have attributes (value of attribute
is in quotes). 
\subsection{Valid XML (that are not needed in HTML)}
\begin{enumerate}
	\item XML is case sensitive
	\item XML needs proper nesting
	\item XML needs closing tags for everything
	\item XML needs a root
	\item has strict naming rules
\end{enumerate}
Elements should be prefered compared to attributes, except for identifier references. Special characters
(eg pointy brackets) need to be escaped in entities.

Can express logical structure of XML in tree/box form. 
There are various corpus online availble in XML format, eg TEI sample spoken text
\subsection{DTD - Document type definition}
Defines legal building blocks of XML document, kind of a schematic for producing valid XMLs.

\section{Language model}
For a given vocabulary, could build infinitely many sentences. Could assume probability of every word
independent of everything else $\rightarrow$ low probability for long sentences (no matter if it makes
sense or not). Language model tries to statistically estimate probability of a word sequence.
\subsection{Counting words}
First part we have to solve. Same word may have different shapes (microsoft, micro\$oft) or
not the same meaning (Mr. Brown, brown fox). Also dates abbrevations make problems or words 
separated by hyphen. 

\subsection{Sentence Segmentation}
Need to find way to segment sentences (different possible regex). Period is easiest, but only 93\% correct. 
Need to also filter fractions/percents and abbrevations to have better results. Additionally direct speech.

\subsection{N-gram Model}
Define \emph{types} as distinct words in a text, \emph{tokens} as all words/symbols in text. 
Words have usually specific context they're used in, can be expressed with probability of n-gram.
Use bigram as approximation for
\begin{equation}
	p(w_i | w_1, w_2, w_3, \ldots w_{i-1}) = p(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
\end{equation}
or more $w_{i-n}$ for n-grams. Above is maximum likelihood method for estimation.

\subsection{Smoothing techniques}
Problem with sparse data, many things may have 0 occurences $\rightarrow$ probability zero (not realistic). 
Various smoothing techniques to improve behaviour
\paragraph{Laplace smoothing}
Add one to every occurence. Things not occuring have occurence 1 then. Needs also
be done for normalization (frequency, divide by number tokens + types).
Very simple technique, but too much shifted towards unseen n-grams, all smoothed the same way. 
Could be improved by adding $\lambda = 0.01$ instead of 1. Can test different $\lambda$ with cross
validation.

\paragraph{Good-Turing Smoothing}
Frequency of n-gram is set to
\begin{equation}
	c* = (c + 1) \cdot \frac{N_{c+1}}{N_c}
\end{equation}
with $N_i$ as the number of tokens we have seen $i$ times and $c$ as frequency of occurence. 
Could e.g. do this only for $c < 10$ bc only reliable for high $N_c$. Probability is then estimated as
\begin{equation}
	 p(w_i | w_{i-1}) = \frac{c^*(w_{i-1}, w_i)}{C(w_{i-1})}
\end{equation}
\paragraph{Linear interpolation}
Linear interpolate between tri, bi and unigram. Weights should sum up to 1, be between 0 and 1.
Weights are chosen according to context (e.g. if lots of data or not many singletons, trigram better). 
\paragraph{Dirichlet Smoothing}
Given model $B$ that gives us probability for word $w_i$, we can estimate p with
\begin{equation}
	p(w_i|B) = \frac{C(w_i)}{N + \mu} + \frac{\mu}{N + \mu}\cdot p_B(w_i)
\end{equation}
First part is direct estimation of probability, second part is under model $B$. Parameter $\mu$ gives
importance of background model.

\section{Word Distributions, Zipf's Law and Word Association}
Usually, number of types (vocabulary) bigger than number of tokens, bc words used multiple times.
Can now model the word distribution (with constant for given auther inside given genre).
\subsection{Zipf's Law}
Regularity, not strict law. Frequency of word $f(w)$ is related to inverse rank $z$ (when ordered by freq)
\begin{equation}
	f(w) = \frac{c}{z^\alpha} = c \cdot z^{-\alpha}
\end{equation}
If we take log on both sides, might end up mor useful bc no more exponent.
\section{Spell checking, 24. March 2014}

\subsection{Damereau-Levenstein metric}
\label{sub:levenstein}
Tries to rank similarity/difference between two tokens: Add up number of
\begin{itemize}
	\item Swappings
	\item Insertions
	\item Deletions
\end{itemize}
to go from String $a$ to String $b$. We build up matrix $D$ in the following way:

\begin{equation}
	\qquad D_{a,b}(i,j) = \begin{cases}
  \max(i,j) & \text{ if} \min(i,j)=0, \\
  \min \begin{cases}
          D_{a,b}(i-1,j) + 1 \\
          D_{a,b}(i,j-1) + 1 \\
          D_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} & \text{ otherwise.}
\end{cases}
\end{equation}
Then we can take the value at the bottom right as distance.
% subsection levenstein (end)

\subsection{Probabilistic methods}
\label{sub:probabilistic_methods}

For a misspelled token $t$, we can find order the canditates by using $f(c)$ of some 
candidate $c$ by evaluating
\begin{equation}
	f(c) = p(c) \cdot p(c|t)
\end{equation}
$p(c)$ models the probability of the candidate (can be easily modeled, apply some smoothing)
, $p(c|t)$ models the misspellings (noise). 
\subsubsection{Modeling misspellings}
\label{ssub:model_misspellings}
Build up confusion matrix that models the probability, some character $q$ was misplaced with some other character $r$. Use this to evaluate probability of some misspelling $t$ of our word $c$. Confusion matrix may be learnt, personalized or device specific. Much better would be to take context into account, too!
% subsubsection model_misspellings (end)

% subsection probabilistc_methods (end)

\subsubsection{Evaluation/Conclusion}
Many errors go undetectd, e.g. grammatical errors, homophones, etc.

\subsection{Markov Models}
\label{sec:markov_models}
Graph with nodes/states $S$, fully connected. Every edge has a certain probability 
(may be 0). Outgoing/incoming probabilities must sum up to 1. Probabilities are displayable 
as matrix $P$. Every path through the graph (Observed sequence $X$) does have a certain probability ($x_n$ is the state
at some time $n$, state changes every timestep (may change to same again)):
\begin{equation}
	p(x_0 = s_{j0}) \cdot p(x_1 = s_{j1} |x_0 = s_{j0}) \cdot p(x_2 = s_{j2} | x_0 = s_{j0}, x_1 = s_{j1})\ldots
\end{equation}
As simplification, we usually assume that the probability of the 
next state is only dependent on the current state (\emph{memoryless property, markov property}). This probability can be easily read from the matrix constructed before.
We also assume that the probabilities don't change with time (are the same for every $x_n$).

Initial distribution: all states have the same probability
% section markov_models (end)

\section{Hidden Markov models HMM, 31. March 2014}
In a hidden Markov model, the states $S$ are not directly observable. Transitions are visible
by emitting symbols (either \emph{arc-emission} HMM or \emph{state-emission} HMM).
So for a HMM we have further
\begin{itemize}
	\item $K$ - set of possible symbols
	\item $O$ - set of observed symbols
	\item $B, b_{ijk}$ probability of emitting symbol $k$ when when transitioning from 
	$s_i$ to $s_j$.
\end{itemize}

See urn-drawing-genie example. Genie is only telling color of ball drawn, but not what urn
it was drawn from. 

\subsection{Crazy drink machine}
Probability of Sequence O = \{ Limonade, Water \} is equal to summing up all possible ways
through a tree of possibilities. See slides.

Other example with O =  \{ Coffee, Coffee \}:
\begin{eqnarray*}
	p(cof, cof | cf; cf; cf) = 0.6 \cdot 0.7\cdot 0.6 \cdot 0.7 \\
	p(cof, cof | cf; cf; wp) = 0.6 \cdot 0.7\cdot 0.6\cdot0.3 \\
	p(cof, cof | cf; wp; cf) = 0.6 \cdot 0.3\cdot 0.1\cdot 0.5 \\
	p(cof, cof | cf; wp; wp) = 0.6 \cdot 0.3\cdot 0.1\cdot 0.5
\end{eqnarray*}
All possible sequences are hard to compute! Use dynamic programming. Can do this forward
and backward. Forward lets you set start-state of HMM, backward lets you set finishing
state of HMM.

We can also retrieve the most likely path through HMM with ... stuff naive algorithm does
 not take probabilities of state changes into account! For correct result, we need to
 apply \emph{Viterbi algorithm}.
 
\section{Parsing natural language, structure of sentence}
\subsection{Motivation}
Can be used for 
\begin{itemize}
	\item grammar checkin, aka spell check
	\item Question Answering (QA), building automatic systems for answering real questions.
	\item Information extraction (IE), e.g. from newspaper everything concerning...
\end{itemize} 
For this, we need formulate formal language, consisting of strings of terminal symbols,
grammar as set of rules on this language.

\subsection{Chomsky Hierarchy}
Various different grammars possible. Sentences can be generated using such a grammer or \emph{analyzed}!

\paragraph{Regular} consists of nonterminal $\rightarrow$ terminal | teriminal nonterminal (e.g. $a^nb^m$)
\paragraph{Context-free} nonterminal $\rightarrow$ anything
\paragraph{Context-sensitive} more nonterminals on lighthand side or right hand side
\paragraph{Recursively enumerable} \emph{no constraint here}, hard to evaluate by a computer.

\subsection{Grammar categories}
Number of non-terminal categories? Usually only 8, divided into two classes
\paragraph{Closed classes} pronouns, determiners, conjunctions, prepositions (no terms can
be added here)
\paragraph{Open classes} nouns, verbs, adverbs (only, rather, ..), adjectives (new terms
can be created, assembled)

\subsection{Morphology}
Changes to words, e.g. suffixes. There are many ambiguities, not sure to what class
word belongs, e.g. round may be adj/verb etc.

Usually we do this dictionary based, take cannonical form of every word and save class in
dictionary, use it to look up. Also include some flexion rules for every word, so it can
be verified if search term can be generated from this canonical form. When looking up, just
try out all morphological rules available

We can play around with a restricted vocabulary (Wumpus grammar). 

\subsection{Shallow parsing}
For performance baseline. Tag every word with its most common POS, disregarding other
possibilities. Leads to 75\% success rate for french. By adding rules you make
a \emph{deep parser} that has up to 97\% success rate.

\subsection{Deep parsing}
Word order can be used as hint for function of a word.

\subsection{Parsing strategies}
Either top down (expectation driven, tree from starting $S$) or bottom up (data driven).

\subsubsection{Top down parsing}
Have some rules, put in full sentence, split it more and more up until you end at
terminal symbol with every rule. There are fail states that need trace backs once in a while.

\subsubsection{Bottom up parsing}
Apply rules greadily to every word from the left, build up until you only have one node
left (root). May have fail states again, which restrict grammar to end up with root
node before whole sentence is parsed.

\subsubsection{Chart parsing}
Most efficient. Try everything, but don't try it more than once. Idea is to combine best 
of top-down and bottom-up. Interprete everything as graph problem: Nodes are word boundaries,
define arcs that are basically part sentences. Assemble tree that has no overlapping
arcs.

\section{Machine learning in NLP (text categorization, classification)}
Most prominent example: filtering spam in emails. For standard classification problem
(supervised learning), we usually have
\begin{enumerate}
	\item predefined categories $C$
	\item a set of labeled documents $D$ (training set)
\end{enumerate}
Now the only thing left is to define a feature vector for a document. Many applications 
in text filtering (for news), author identification (or just information about author),
document indexing etc.

Women use more pronouns (I), men use more nouns. 

\subsection{Classifiers}
\begin{itemize}
	\item Rule based: Some experts produces rules to classify, just hardcode these. Usually
	an expert can not provide such. Knowledge aquisition is bottleneck, may not be 
	able to classify unseen examples.
	\item TF-IDF
	\item Rocchio method
	\item Example based classifiers (k-nearest-neighbour KNN)
\end{itemize}
\subsection{Feature vectors}
Can not extract semantic of document easily, but need to build a compact text representation anyway. Use a set of weighted term $t_k$. Terms can be found e.g. by bag
of words/ tf-idf:
\begin{equation}
	w_{kj} = \frac{tf_{kj}\cdot idf_k}{\sqrt{\sum_i (tf_{ij}\cdot idf_i)^2}}
\end{equation}
with $t_k$ a term in document $d_j$. $t_{kj}$ is the frequency of the term $k$ in 
document $j$. $idf_k = \log(n/df_k)$ is the log of the
inverse of the frequency of the term $k$ in 
all documents. 

To improve results we can do stemming to reduce number of different terms (e.g. make
computing and computer the same word). Or remove stop words. May only apply to part
of document (e.g. abstract).

This leads to huge vectors. We usually need
to reduce dimensionality. 
\paragraph{Term extraction:} Could apply term clustering, then use the centroid (is a synthetic term then).
\paragraph{Term selection:} Only take terms
that have highest $df_k$, e.g. remove
all terms that occure less than k times
in training documents. Alternatively use Mutual Information MI. Remove terms with MI between term and category closest to 0.
This means, this term is a bad feature for describing this category.
Could also use \emph{Information Gain} or \emph{Odds Ratio}. 
A smoothing strategy could be useful here.

To exclude values, you could use different methods to combine score for all categories. E.g. sum up the absolutes, take the max or weight absolute by probability of category and sum up.

\subsection{Maria tips on computation/test questions}
Compute MI for contingency table
\begin{equation}
	MI = \log \frac{p(t_k, c_i)}{p(t_k)\cdot p(c_i)} = \log \frac{a \cdot m}{(a + b) (a + c)}
\end{equation}
The shortcut to the right works on a contingency table in default format. 

\section{Continuation on Text categorization, 5. May 2014}
\subsection{Decision Rule classifier}
If no expert available, just put together everything with if/else labels. E.g. label 'wheat' -> 'farming'. Can also postprocess rules, e.g. filter out contradicting ones. 

\subsection{TF-IDF }
Make vector with TF-IDF of words, compute similarity of two vectors

\subsection{Rocchio Method}
Similar as tf-idf, must different weights? Take weights for all texts and divide by number
of texts in this category, then subtract sum of weights of all negative documents and divide
by number of negative documents. Gives more weight to more important terms. Formula will be
given if there is any question related.

\subsection{K-Nearest Neighbour}
Find nearest neighbour of learning set, apply same label. Could also retrieve k-nearest
neighbour and label with most common label. For taking nearest neighbour, could use cosine
similarity of TF-IDF.

\subsection{Evaluating classifier}
\subsubsection{Precision, Recall}
Precision is the number of true positive over the number of positives classified
(e.g. true spam classified as spam over everything classified as spam (not very important).
Recall is the number of true poisitives classified correctly over the all true positives.

Can be combined as F value:
\begin{equation}
	F_{\beta i} = \frac{(\beta^2 + 1)\cdot prec_i \cdot recall_i}{\beta^2 \cdot prec_i \cdot recall_i}
\end{equation}
\paragraph{Multicategory precision/recall}
Can compute precision/recall for every single category. Can either do micro-averaging:
\begin{equation}
	prec_{all} = \frac{TP_{all}}{TP_{all} + FP_{all}}
\end{equation}
But this might be tainted, if only one single class is very bad. Macro-averaging would simply be taking the average over all recalls (does not take into
account, that one class might be much more prevalent than others). 

\subsubsection{K-fold cross validation}
Divide training set into k subsets, train on k-1 subsets, do validation with the one left out. Repeat k times with every subset used for validation once.
\paragraph{Leave-one-out cross-validation}
Only use 1 sample for validation, rest for training. Is the same as k-fold cross validation
with k=n with n the number of training samples.

\subsection{Example of opinion mining}
Need to find out if comment is positive, negative or neutral. Needs to cope with mixed
opinions, sarcasm etc. Usually, we define positive and negative words, count occurence. 
Need to handle negative case, double negative, noisy language (spelling).

\section{Example authorship attribution, }
Learn on text of some $n$ authors, try to classify new text. 
\paragraph{Closed-Set} Can only be one of the authors learnt
\paragraph{Open-Set} Can be in set or new author
\paragraph{Verification} Determine, if attributed author is correct
\paragraph{Profiling} Determine some attributes of author (sex, age, education...)
\paragraph{Collaborative} Were there multiple authors?
Use style of \textbf{author}, which depends on social group, gender, age, education, \textbf{period in
time}, topic, etc...
\subsection{Single measurement}
Count frequency of letters (or words) to describe author. Surprisingly, this somehow works!
Also possible use vocabular richness, measured e.g. as 
\paragraph{Guiraud}
\begin{equation}
	R = \frac{V}{sqrt(N)}
\end{equation}
\paragraph{Simpson's index}
\begin{equation}
	D = \frac{\sum r(r-1) V_r}{N(N-1)}
\end{equation}
with $V_r$ as number of types that occur $r$ times, $N$ the number of tokens.
\subsection{Stylistic approaches}
Single measurement not enough, to something more sophisticated. Take more evidence into
account. 
\subsubsection{Burrows' Delta}
Take $n$ most frequent words, do Z-Index with them etc.
\subsubsection{Kullback-Leibler Divergence}
Define set of frequent words for the text's language, e.g. use stopword list. 
Define its probability in quert text
 $q$ and for authors texts $a_i$. Take distance via
\begin{equation}
	KLD(Q|| A_j) = \sum_i q(t_i) \cdot log_2 \left( \frac{q(t_i}{a_j(t_i)} \right)
\end{equation}
Low KLD indicated probable author. To prevent invalid log/division, make some special cases.
Is not normalized, so may have values larger than 1. Can be combined with smoothing.
\subsubsection{Z-Score}
As before, Obama vs Caine, take summed squared difference in z-score between author and 
normal language. Can make vector of some most frequent terms, compare with other authors.

\section{Cryptography, 12. May. 2014}
\subsection{Steganography}
Add watermark to image. Can be used to transmit hidden message. Does have the drawback, that
as soon the algorithm is known, everyone can read the message. Only weak security!
\subsection{Substitution Cypher}
Reorder letters according to some pattern. E.g. Caesar, shift everything by 3 letters. 
Many possible ways to generate substitution patterns, key space is very big. Exhaustive 
search there can not yield good results. But can be evaluated statistically (cryptanalysis).

\subsection{Cryptanalysis}
Take frequencies of default test, apply to frequencies of cypher text. Then we have probable
mapping to decypher texts.

\subsection{Homophonic substitution}
Instead of substituting one letter with one letter, we could replace a character by multiple
characters and a one-to-many mapping. Makes statistical analysis much harder.
\subsection{Polyalphabetic substitution}
E.g. Vigen√®re cypher, repeat key until it matches lenght of plain text, add up plain text
character and key text character (modulo 26) equals cypher text. This way, some letter B
may once be encrypted as C and another time as D etc.

\subsubsection{Cryptanalysis (Kasiski method)}
Problem here is, that repetition in plain text might be visible in cypher text. From this
the key size can be deferred. Works for short key sentences that repeat a lot. For longer
sentences, it is possible to place common words in key, look at the decrypted message and do
som validation where it seems correct.
\subsection{Enigma}
Still monoalphabetic substitution, but with multirotor substitution scheme. Need to set initial
rotor setting, then automatically moving forward. 

\subsection{One time pad for perfect secrecy}
Use key only once, do XOR on byte level. Difficult to apply in practice, since hard to exchange
key. 
\subsection{Block cypher}
Fixed size of 128-bit, input is 128 bit, output is 128 bit. Lenght of plain text can not be
deferred from cypher text anymore. E.g. used by DES.

\section{Summary - possible questions, 18. May 2014 }
\subsection{Introduction}
\subsubsection{Why is NLP difficult? What are the differences between parsing artificial language (Java) and 
a real one?}
\begin{itemize}
	\item Vocabulary not completely known, not all meanings of a single word are known (everything is 
changing/shifting). Infinite number of possible sentences.
	\item Needs high tolerance for errors.
	\item Implicit elements, meaning deferred from last sentence etc. Mr, Prime minister e.g. 
	can be same person. Idiomatics meaning are possible. 
	\item Ambiguity! Many ways to parse single sentence.
\end{itemize}
Can be somewhat handled with regex. 
\subsubsection{XML vs HTML}
elements vs entities, XML more strict. Turn DTD (visualization with rectangles) 
into xml and vice versa.
\subsection{Spelling detection and Correction}
Type of errors: transposition (swap two letters), remove one letter, replace one letter, add extra letter. Error can be measured with dynamic programming (levenshtein distance). Do that
with matrix for example. 

soundex (make classes of letters with similar sound, tries to detect phoenetic
errors)

\subsection{Language model}
Zipfs model, make rank of frequencies, turns out it's logarithmic. 
Mutual information (not needed to learn by heart). Can compute classical example of 4x4 table
(IT, not IT, Obama, not Obama).
What is language model? Statistical model of language, use bigrams/trigrams. 

\subsection{Parsing sentences}
Goal, full vs shallow, bottom up (start from left, assemble until only one node) vs top down (start with full sentence, chop it apart) vs chat parsing. 

\subsection{Text classification}
Representation of text (stems), use nearest neighbor or so. Evaluate quality of system: precision and recall). 

\subsection{Authorship attribution}
Select features (e.g. words, most common words...) and compute some distance. Kullback leibler 
distance, what is meaning of zero (Same distribution in $Q$ and $A_j$), what is meaning of infinity? Negative is not possible. Certainly either MI or Kullback Leibler will be in test!

\subsubsection{Naive Bayes}
How much is word associated to auther? Can be computed here. wat?

\subsection{Memorize}
Need to know how to compute Zipfs law, MI. Markov chain: Coffee-coffe, how probable is that?
\end{document}
