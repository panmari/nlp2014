\documentclass[11pt]{article}
%Gummi|065|=)
\title{\textbf{Natural language processing}}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\author{Stefan Moser}
\date{}
\begin{document}

\maketitle

\section{Spell checking, 24. March 2014}

\subsection{Damereau-Levenstein metric}
\label{sub:levenstein}
Tries to rank similarity/difference between two tokens: Add up number of
\begin{itemize}
	\item Swappings
	\item Insertions
	\item Deletions
\end{itemize}
to go from String $a$ to String $b$. We build up matrix $D$ in the following way:

\begin{equation}
	\qquad D_{a,b}(i,j) = \begin{cases}
  \max(i,j) & \text{ if} \min(i,j)=0, \\
  \min \begin{cases}
          D_{a,b}(i-1,j) + 1 \\
          D_{a,b}(i,j-1) + 1 \\
          D_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} & \text{ otherwise.}
\end{cases}
\end{equation}
Then we can take the value at the bottom right as distance.
% subsection levenstein (end)

\subsection{Probabilistic methods}
\label{sub:probabilistic_methods}

For a misspelled token $t$, we can find order the canditates by using $f(c)$ of some 
candidate $c$ by evaluating
\begin{equation}
	f(c) = p(c) \cdot p(c|t)
\end{equation}
$p(c)$ models the probability of the candidate (can be easily modeled, apply some smoothing)
, $p(c|t)$ models the misspellings (noise). 
\subsubsection{Modeling misspellings}
\label{ssub:model_misspellings}
Build up confusion matrix that models the probability, some character $q$ was misplaced with some other character $r$. Use this to evaluate probability of some misspelling $t$ of our word $c$. Confusion matrix may be learnt, personalized or device specific. Much better would be to take context into account, too!
% subsubsection model_misspellings (end)

% subsection probabilistc_methods (end)

\subsubsection{Evaluation/Conclusion}
Many errors go undetectd, e.g. grammatical errors, homophones, etc.

\subsection{Markov Models}
\label{sec:markov_models}
Graph with nodes/states $S$, fully connected. Every edge has a certain probability 
(may be 0). Outgoing/incoming probabilities must sum up to 1. Probabilities are displayable 
as matrix $P$. Every path through the graph (Observed sequence $X$) does have a certain probability ($x_n$ is the state
at some time $n$, state changes every timestep (may change to same again)):
\begin{equation}
	p(x_0 = s_{j0}) \cdot p(x_1 = s_{j1} |x_0 = s_{j0}) \cdot p(x_2 = s_{j2} | x_0 = s_{j0}, x_1 = s_{j1})\ldots
\end{equation}
As simplification, we usually assume that the probability of the 
next state is only dependent on the current state (\emph{memoryless property, markov property}). This probability can be easily read from the matrix constructed before.
We also assume that the probabilities don't change with time (are the same for every $x_n$).

Initial distribution: all states have the same probability
% section markov_models (end)

\section{Hidden Markov models HMM, 31. March 2014}
In a hidden Markov model, the states $S$ are not directly observable. Transitions are visible
by emitting symbols (either \emph{arc-emission} HMM or \emph{state-emission} HMM).
So for a HMM we have further
\begin{itemize}
	\item $K$ - set of possible symbols
	\item $O$ - set of observed symbols
	\item $B, b_{ijk}$ probability of emitting symbol $k$ when when transitioning from 
	$s_i$ to $s_j$.
\end{itemize}

See urn-drawing-genie example. Genie is only telling color of ball drawn, but not what urn
it was drawn from. 

\subsection{Crazy drink machine}
Probability of Sequence O = \{ Limonade, Water \} is equal to summing up all possible ways
through a tree of possibilities. See slides.

Other example with O =  \{ Coffee, Coffee \}:
\begin{eqnarray*}
	p(cof, cof | cf; cf; cf) = 0.6 \cdot 0.7\cdot 0.6 \cdot 0.7 \\
	p(cof, cof | cf; cf; wp) = 0.6 \cdot 0.7\cdot 0.6\cdot0.3 \\
	p(cof, cof | cf; wp; cf) = 0.6 \cdot 0.3\cdot 0.1\cdot 0.5 \\
	p(cof, cof | cf; wp; wp) = 0.6 \cdot 0.3\cdot 0.1\cdot 0.5
\end{eqnarray*}
All possible sequences are hard to compute! Use dynamic programming. Can do this forward
and backward. Forward lets you set start-state of HMM, backward lets you set finishing
state of HMM.

We can also retrieve the most likely path through HMM with ... stuff naive algorithm does
 not take probabilities of state changes into account! For correct result, we need to
 apply \emph{Viterbi algorithm}.
 
\section{Parsing natural language, structure of sentence}
\subsection{Motivation}
Can be used for 
\begin{itemize}
	\item grammar checkin, aka spell check
	\item Question Answering (QA), building automatic systems for answering real questions.
	\item Information extraction (IE), e.g. from newspaper everything concerning...
\end{itemize} 
For this, we need formulate formal language, consisting of strings of terminal symbols,
grammar as set of rules on this language.

\subsection{Chomsky Hierarchy}
Various different grammars possible. Sentences can be generated using such a grammer or \emph{analyzed}!

\paragraph{Regular} consists of nonterminal $\rightarrow$ terminal | teriminal nonterminal (e.g. $a^nb^m$)
\paragraph{Context-free} nonterminal $\rightarrow$ anything
\paragraph{Context-sensitive} more nonterminals on lighthand side or right hand side
\paragraph{Recursively enumerable} \emph{no constraint here}, hard to evaluate by a computer.

\subsection{Grammar categories}
Number of non-terminal categories? Usually only 8, divided into two classes
\paragraph{Closed classes} pronouns, determiners, conjunctions, prepositions (no terms can
be added here)
\paragraph{Open classes} nouns, verbs, adverbs (only, rather, ..), adjectives (new terms
can be created, assembled)

\subsection{Morphology}
Changes to words, e.g. suffixes. There are many ambiguities, not sure to what class
word belongs, e.g. round may be adj/verb etc.

Usually we do this dictionary based, take cannonical form of every word and save class in
dictionary, use it to look up. Also include some flexion rules for every word, so it can
be verified if search term can be generated from this canonical form. When looking up, just
try out all morphological rules available

We can play around with a restricted vocabulary (Wumpus grammar). 

\subsection{Shallow parsing}
For performance baseline. Tag every word with its most common POS, disregarding other
possibilities. Leads to 75\% success rate for french. By adding rules you make
a \emph{deep parser} that has up to 97\% success rate.

\subsection{Deep parsing}
Word order can be used as hint for function of a word.

\subsection{Parsing strategies}
Either top down (expectation driven, tree from starting $S$) or bottom up (data driven).

\subsubsection{Top down parsing}
Have some rules, put in full sentence, split it more and more up until you end at
terminal symbol with every rule. There are fail states that need trace backs once in a while.

\subsubsection{Bottom up parsing}
Apply rules greadily to every word from the left, build up until you only have one node
left (root). May have fail states again, which restrict grammar to end up with root
node before whole sentence is parsed.

\subsubsection{Chart parsing}
Most efficient. Try everything, but don't try it more than once. Idea is to combine best 
of top-down and bottom-up. Interprete everything as graph problem: Nodes are word boundaries,
define arcs that are basically part sentences. Assemble tree that has no overlapping
arcs.
\end{document}
